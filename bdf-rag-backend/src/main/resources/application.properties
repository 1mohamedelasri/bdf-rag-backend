spring.application.name=bdf-rag-backend

# -----------------------------
# DB Config
# -----------------------------
spring.datasource.url=jdbc:postgresql://bdf-db:5432/bdf_vector_db
spring.datasource.username=bdf_admin
spring.datasource.password=securepassword
spring.jpa.hibernate.ddl-auto=update

# -----------------------------
# Vector Store (PGVector)
# -----------------------------
spring.ai.vectorstore.pgvector.initialize-schema=true
spring.ai.vectorstore.pgvector.index-type=HNSW
spring.ai.vectorstore.pgvector.distance-type=COSINE_DISTANCE

# IMPORTANT: correct key is "dimensions" (plural)
spring.ai.vectorstore.pgvector.dimensions=768

# DEV ONLY: this fixes your current issue by recreating the table with vector(768).
# After the first successful start+ingestion, set this back to false, otherwise you'll lose data on every restart.
spring.ai.vectorstore.pgvector.remove-existing-vector-store-table=true

# Optional (keeps batching sane)
spring.ai.vectorstore.pgvector.max-document-batch-size=10000

# -----------------------------
# Ollama (Docker)
# -----------------------------
spring.ai.ollama.base-url=http://bdf-ollama:11434

# Enable Ollama models (new Spring AI config style)
spring.ai.model.chat=ollama
spring.ai.model.embedding=ollama

# Correct properties for model selection
spring.ai.ollama.chat.options.model=llama3
spring.ai.ollama.embedding.options.model=nomic-embed-text

# Optional: auto-pull missing models (nice for fresh environments)
# spring.ai.ollama.init.pull-model-strategy=when_missing
# spring.ai.ollama.init.timeout=60s
# spring.ai.ollama.init.max-retries=1

# -----------------------------
# Upload limits
# -----------------------------
spring.servlet.multipart.max-file-size=20MB
spring.servlet.multipart.max-request-size=20MB
